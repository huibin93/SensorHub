"""
æ–‡ä»¶ç›¸å…³ API ç«¯ç‚¹æ¨¡å—;

æœ¬æ¨¡å—æä¾›ä¼ æ„Ÿå™¨æ–‡ä»¶çš„ CRUD æ“ä½œã€ä¸Šä¼ ã€ä¸‹è½½ã€è§£æç­‰ API ç«¯ç‚¹;
"""
from fastapi import APIRouter, Depends, UploadFile, File, Form, HTTPException, Query, Header, BackgroundTasks
from starlette.background import BackgroundTask
from pathlib import Path
from sqlmodel import Session
from typing import List, Optional, Any
import uuid
import json
from datetime import datetime

from app.api import deps
from app.schemas import api_models
from app.crud import file as crud
from app.services.storage import StorageService
from app.services.parser import ParserService
from app.services.metadata import parse_filename, ensure_test_types_exist
from app.models.sensor_file import SensorFile, PhysicalFile
from app.core.logger import logger
from app.core.database import engine

router = APIRouter()


@router.get("/stats", response_model=api_models.StatsResponse)
def get_stats(session: Session = Depends(deps.get_db)) -> api_models.StatsResponse:
    """
    è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯;

    Returns:
        StatsResponse: åŒ…å«æ–‡ä»¶æ€»æ•°ã€ä»Šæ—¥ä¸Šä¼ æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯;
    """
    return crud.get_stats(session)


@router.get("/files", response_model=api_models.PaginatedFilesResponse)
def get_files(
    page: int = 1,
    limit: int = 20,
    search: Optional[str] = None,
    device: Optional[str] = None,
    status: Optional[str] = None,
    sort: str = "-uploadTime",
    session: Session = Depends(deps.get_db)
) -> api_models.PaginatedFilesResponse:
    """
    è·å–æ–‡ä»¶åˆ—è¡¨(åˆ†é¡µ);

    Args:
        page: é¡µç ,ä» 1 å¼€å§‹;
        limit: æ¯é¡µæ•°é‡;
        search: æœç´¢å…³é”®è¯(æ–‡ä»¶åæˆ–å¤‡æ³¨);
        device: è®¾å¤‡ç±»å‹ç­›é€‰;
        status: çŠ¶æ€ç­›é€‰;
        sort: æ’åºå­—æ®µ,å‰ç¼€ "-" è¡¨ç¤ºé™åº;

    Returns:
        PaginatedFilesResponse: åˆ†é¡µçš„æ–‡ä»¶åˆ—è¡¨;
    """
    try:
        skip = (page - 1) * limit
        files, total = crud.get_files(session, skip, limit, search, device, status, sort)
        return {
            "items": files,
            "total": total,
            "page": page,
            "limit": limit,
            "totalPages": (total + limit - 1) // limit if limit > 0 else 1
        }
    except Exception as e:
        import traceback
        logger.error(f"Error in get_files: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/files/check")
def check_file(
    hash: Optional[str] = Query(None), # Hash å˜ä¸ºå¯é€‰
    filename: Optional[str] = Query(None), 
    size: Optional[int] = Query(None), # æ–°å¢å¤§å°å‚æ•°
    session: Session = Depends(deps.get_db)
):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ (Pre-check & ç§’ä¼ );
    
    Args:
        hash: æ–‡ä»¶ Hash (MD5), å¯é€‰.
        filename: æ–‡ä»¶å, å¯é€‰.
        size: æ–‡ä»¶å¤§å°, å¯é€‰.
        
    Returns:
        dict: {exists: bool, exact_match: bool, file: ...}
    """
    # 0. å¿«é€Ÿå‰ç½®æ£€æŸ¥ (Fast Check): åŒåä¸”åŒå¤§å°
    if filename and size is not None:
        fast_match = crud.get_file_by_name_and_size(session, filename, size)
        if fast_match:
             return {
                 "exists": True, 
                 "exact_match": True, 
                 "fileId": fast_match.id, 
                 "filename": fast_match.filename,
                 "message": "Fast Check: File with same name and size exists."
             }

    # 1. å¦‚æœæä¾›äº† Hash å’Œæ–‡ä»¶åï¼Œæ£€æŸ¥ä¸¥æ ¼åŒ¹é…
    if hash and filename:
        exact = crud.get_exact_match_file(session, hash, filename)
        if exact:
             return {
                 "exists": True, 
                 "exact_match": True, 
                 "fileId": exact.id, 
                 "filename": exact.filename,
                 "message": "Strict Check: File with same content and name already exists."
             }

    # 2. å¦‚æœæä¾›äº† Hashï¼Œæ£€æŸ¥å†…å®¹åŒ¹é… (ç§’ä¼ )
    if hash:
        file = crud.get_file_by_hash(session, hash)
        if file:
            return {
                "exists": True, 
                "exact_match": False,
                "fileId": file.id, 
                "filename": file.filename,
                "message": "File content exists (different name)."
            }
    
    return {"exists": False, "exact_match": False}


# --- Background Tasks ---
def verify_upload_task(file_id: str, md5: str, path: str):
    """
    åå°æ ¡éªŒä»»åŠ¡: æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§å¹¶æ›´æ–° DB Status
    """
    logger.info(f"Starting background verification for file {file_id}")
    try:
        is_valid = StorageService.verify_integrity(Path(path), md5)
        
        with Session(engine) as session:
            if is_valid:
                crud.update_file(session, file_id, {"status": "idle"})
                logger.info(f"File {file_id} verified successfully.")
            else:
                msg = "Integrity Check Failed"
                crud.update_file(session, file_id, {"status": "error", "notes": msg, "error_message": msg})
                logger.error(f"File {file_id} validation failed.")
    except Exception as e:
        logger.error(f"Error in verify_upload_task: {e}")
        with Session(engine) as session:
            msg = f"Verification Error: {str(e)}"
            crud.update_file(session, file_id, {"status": "error", "notes": msg, "error_message": msg})


@router.post("/files/upload", response_model=Any) # Return Any to support flexible JSON
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    md5: str = Form(...),
    filename: str = Form(..., description="Original filename"),
    original_size: int = Form(...),
    deviceType: Optional[str] = Form("Unknown"),
    session: Session = Depends(deps.get_db)
) -> Any:
    """
    æµå¼ä¸Šä¼  Zstd å‹ç¼©æ–‡ä»¶ (æ¥å£ v2);
    å‰ç«¯å·²å®Œæˆå‹ç¼©å’Œ MD5 è®¡ç®—ã€‚åç«¯ç›´æ¥è½ç›˜å¹¶å¼‚æ­¥æ ¡éªŒã€‚
    """
    # 1. æ£€æŸ¥ç‰©ç†æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (ç§’ä¼ æ ¸å¿ƒé€»è¾‘)
    existing_phy = crud.get_physical_file(session, md5)
    expected_raw_path = StorageService.get_raw_path(md5)
    
    if existing_phy and expected_raw_path.exists():
         # --- å‘½ä¸­ç§’ä¼  (Physical Deduplication) ---
         logger.info(f"Instant upload (deduplication) for {filename} ({md5})")
         
         # ä¸¥è‹›å»é‡æ£€æŸ¥: å¦‚æœå·²å­˜åœ¨ åŒåä¸”åŒHash çš„ SensorFile, ç›´æ¥è¿”å›è¯¥è®°å½•
         exact_match = crud.get_exact_match_file(session, md5, filename)
         if exact_match:
             logger.info(f"Exact match found for {filename} ({md5}). Skipping creation.")
             return {
                 "code": 200,
                 "data": {
                     "file_id": exact_match.id,
                     "status": exact_match.status,
                     "saved_path": str(expected_raw_path),
                     "is_duplicate": True
                 },
                 "message": "æ–‡ä»¶å·²å­˜åœ¨ (æ— éœ€é‡å¤ä¸Šä¼ )"
             }

         # æ£€æŸ¥è§£æçŠ¶æ€ (Smart Status)
         processed_dir = StorageService.get_processed_dir(md5)
         initial_status = "idle"
         if processed_dir.exists() and any(processed_dir.iterdir()):
             initial_status = "processed"
         
         # åˆ›å»ºæ–°çš„ SensorFile (æŒ‡å‘åŒä¸€ä¸ª Hash, ä½†æ–‡ä»¶åä¸åŒ)
         file_id = str(uuid.uuid4())
         
         # è®¡ç®—æ–‡ä»¶ååç¼€
         name_suffix = crud.get_next_naming_suffix(session, filename)
         
         # æ˜¾ç¤ºå¤§å°
         if original_size < 1024:
             size_str = f"{original_size} B"
         elif original_size < 1024 * 1024:
             size_str = f"{original_size / 1024:.1f} KB"
         elif original_size < 1024 * 1024 * 1024:
             size_str = f"{original_size / (1024 * 1024):.1f} MB"
         else:
             size_str = f"{original_size / (1024 * 1024 * 1024):.1f} GB"
         
         new_sf = SensorFile(
             id=file_id,
             file_hash=md5,
             filename=filename, # ä½¿ç”¨æ–°ä¸Šä¼ çš„æ–‡ä»¶å
             deviceType=deviceType,
             deviceModel="Unknown",
             size=size_str,
             file_size_bytes=original_size, # ä¿å­˜ Int å¤§å°
             name_suffix=name_suffix,
             uploadTime=datetime.utcnow().isoformat(),
             status=initial_status,
             processedDir=str(processed_dir)
         )
         
         # Parse Metadata (Optional override for deduplication case? 
         # Requirement says "Frontend file entry also needs parsing". 
         # If strict exact match found, we skip return.
         # But here we are creating a NEW SensorFile pointing to OLD physical file.
         # So we SHOULD parse the NEW filename metadata.
         meta = parse_filename(filename)
         new_sf.test_type_l1 = meta.get("test_type_l1", "Unknown")
         new_sf.test_type_l2 = meta.get("test_type_l2", "--")
         new_sf.tester = meta.get("tester", "")
         new_sf.mac = meta.get("mac", "")
         new_sf.collection_time = meta.get("collection_time", "")
         if meta.get("deviceType"):
             new_sf.device_type = meta.get("deviceType")
         
         # Auto-Insert Dictionary
         if meta.get("test_type_l1"):
             ensure_test_types_exist(session, meta.get("test_type_l1"), meta.get("test_type_l2"))

         crud.create_file(session, new_sf)
         
         return {
             "code": 200,
             "data": {
                 "file_id": file_id,
                 "status": initial_status,
                 "saved_path": str(expected_raw_path)
             },
             "message": "ğŸ‰ ç§’ä¼ æˆåŠŸï¼(File exists)"
         }
    
    # 2. ç‰©ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ‰§è¡Œå¸¸è§„ä¸Šä¼ 
    file_id = str(uuid.uuid4())
    
    # 3. æµå¼è½ç›˜ (ä¸è®ºæ˜¯å¦é¦–æ¬¡,éƒ½è¦†ç›–å†™å…¥ä»¥ç¡®ä¿æ–‡ä»¶æ­£ç¡®)
    try:
        save_res = await StorageService.save_zstd_stream(file, md5)
        saved_path = save_res["raw_path"]
        
        # 4. æ›´æ–°/åˆ›å»º DB è®°å½•
        
        # 4.1 PhysicalFile
        phy_file = crud.get_physical_file(session, md5)
        if not phy_file:
            phy_file = PhysicalFile(hash=md5, size=save_res["file_size"], path=saved_path)
            crud.create_physical_file(session, phy_file)
            
        # 4.2 SensorFile
        # è®¡ç®—æ–‡ä»¶ååç¼€
        name_suffix = crud.get_next_naming_suffix(session, filename)
        
        # æ˜¾ç¤ºå¤§å°
        if original_size < 1024:
            size_str = f"{original_size} B"
        elif original_size < 1024 * 1024:
            size_str = f"{original_size / 1024:.1f} KB"
        elif original_size < 1024 * 1024 * 1024:
            size_str = f"{original_size / (1024 * 1024):.1f} MB"
        else:
            size_str = f"{original_size / (1024 * 1024 * 1024):.1f} GB"
            
        new_sf = SensorFile(
            id=file_id,
            file_hash=md5,
            filename=filename,
            deviceType=deviceType,
            deviceModel="Unknown",
            size=size_str,
            file_size_bytes=original_size, # ä¿å­˜ Int å¤§å° 
            name_suffix=name_suffix,
            uploadTime=datetime.utcnow().isoformat(),
            status="unverified",
            processedDir=str(StorageService.get_processed_dir(md5)) # ä½¿ç”¨ Hash
        )
        
        # Parse Metadata
        meta = parse_filename(filename)
        new_sf.test_type_l1 = meta.get("test_type_l1", "Unknown")
        new_sf.test_type_l2 = meta.get("test_type_l2", "--")
        new_sf.tester = meta.get("tester", "")
        new_sf.mac = meta.get("mac", "")
        new_sf.collection_time = meta.get("collection_time", "")
        if meta.get("deviceType"):
             new_sf.device_type = meta.get("deviceType")

        # Auto-Insert Dictionary
        if meta.get("test_type_l1"):
             ensure_test_types_exist(session, meta.get("test_type_l1"), meta.get("test_type_l2"))

        crud.create_file(session, new_sf)
        
        # 5. è§¦å‘åå°æ ¡éªŒ
        background_tasks.add_task(verify_upload_task, file_id, md5, saved_path)
        
        return {
            "code": 200,
            "data": {
                "file_id": file_id,
                "status": "unverified",
                "saved_path": saved_path
            },
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ,æ­£åœ¨æ ¡éªŒ..."
        }
    except Exception as e:
        logger.error(f"Upload processing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))




@router.patch("/files/{file_id}")
def update_file(
    file_id: str,
    update: api_models.FileUpdateRequest,
    session: Session = Depends(deps.get_db)
) -> SensorFile:
    """
    æ›´æ–°æ–‡ä»¶ä¿¡æ¯;

    Args:
        file_id: æ–‡ä»¶ ID;
        update: è¦æ›´æ–°çš„å­—æ®µ;

    Returns:
        SensorFile: æ›´æ–°åçš„æ–‡ä»¶å¯¹è±¡;

    Raises:
        HTTPException: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º 404 é”™è¯¯;
    """
    updated = crud.update_file(session, file_id, update.model_dump(exclude_unset=True))
    if not updated:
        raise HTTPException(status_code=404, detail="File not found")
    return updated


@router.delete("/files/{file_id}")
def delete_file(file_id: str, session: Session = Depends(deps.get_db)) -> dict:
    """
    åˆ é™¤å•ä¸ªæ–‡ä»¶;

    Args:
        file_id: æ–‡ä»¶ ID;

    Returns:
        dict: åˆ é™¤ç»“æœ;
    """
    crud.delete_file_safely(session, file_id)
    # StorageService.delete_file(file_id) # Deprecated, handled in delete_file_safely
    return {"success": True}


@router.post("/files/batch-delete")
def batch_delete(
    request: api_models.BatchDeleteRequest,
    session: Session = Depends(deps.get_db)
) -> dict:
    """
    æ‰¹é‡åˆ é™¤æ–‡ä»¶;

    Args:
        request: åŒ…å«è¦åˆ é™¤çš„æ–‡ä»¶ ID åˆ—è¡¨;

    Returns:
        dict: åˆ é™¤ç»“æœ,åŒ…å«åˆ é™¤æ•°é‡;
    """
    for fid in request.ids:
        crud.delete_file_safely(session, fid)
        # StorageService.delete_file(fid)
    return {"success": True, "deleted": len(request.ids)}


@router.get("/files/{file_id}/structure")
def get_structure(file_id: str, session: Session = Depends(deps.get_db)) -> dict:
    """
    è·å–æ–‡ä»¶ç»“æ„ä¿¡æ¯;

    Args:
        file_id: æ–‡ä»¶ ID;

    Returns:
        dict: æ–‡ä»¶ç»“æ„å…ƒæ•°æ®;

    Raises:
        HTTPException: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º 404 é”™è¯¯;
    """
    file = crud.get_file(session, file_id)
    if not file:
        raise HTTPException(status_code=404, detail="File not found")
    return {
        "fileId": file.id,
        "status": file.status,
        "processedDir": file.processed_dir,
        "contentMeta": file.content_meta or {}
    }


@router.get("/files/{file_id}/data/{key}")
def get_file_data(
    file_id: str,
    key: str,
    limit: int = 1000,
    columns: Optional[str] = None,
    session: Session = Depends(deps.get_db)
) -> dict:
    """
    è·å–æ–‡ä»¶è§£æåçš„æ•°æ®;

    Args:
        file_id: æ–‡ä»¶ ID;
        key: æ•°æ®é”®å(å¯¹åº” Parquet æ–‡ä»¶å);
        limit: è¿”å›è¡Œæ•°é™åˆ¶;
        columns: è¦è¿”å›çš„åˆ—å,é€—å·åˆ†éš”;

    Returns:
        dict: åŒ…å«æ•°æ®æ•°ç»„çš„å­—å…¸;

    Raises:
        HTTPException: æ–‡ä»¶æˆ–æ•°æ®ä¸å­˜åœ¨æ—¶æŠ›å‡º 404 é”™è¯¯;
    """
    file = crud.get_file(session, file_id)
    if not file:
        raise HTTPException(404, "File not found")

    # è¯»å– Parquet æ–‡ä»¶ (ä½¿ç”¨ Hash è·¯å¾„)
    # ä¿®æ­£: processedDir å·²ç»åœ¨ create æ—¶æŒ‡å‘äº† Hash ç›®å½•ï¼Œä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬ä½¿ç”¨ file.file_hash
    # å› ä¸º processedDir å­—æ®µå­˜å‚¨çš„æ˜¯ å­—ç¬¦ä¸² è·¯å¾„ã€‚
    # æœ€å¥½ä½¿ç”¨ Service ç»Ÿä¸€è·å–
    processed_dir = StorageService.get_processed_dir(file.file_hash)
    pq_path = processed_dir / f"{key}.parquet"

    if not pq_path.exists():
        raise HTTPException(404, f"Data not found: {key}")

    try:
        import pyarrow.parquet as pq
        table = pq.read_table(pq_path)

        # ç­›é€‰åˆ—
        if columns:
            cols = columns.split(',')
            existing = table.column_names
            cols = [c for c in cols if c in existing]
            if cols:
                table = table.select(cols)

        # é™åˆ¶è¡Œæ•°
        df = table.to_pandas()
        if limit > 0:
            df = df.head(limit)

        data = json.loads(df.to_json(orient="records"))
        return {"data": data}
    except Exception as e:
        raise HTTPException(500, f"Error reading data: {str(e)}")


@router.get("/files/{file_id}/download")
def download_file(file_id: str, session: Session = Depends(deps.get_db)):
    """
    ä¸‹è½½åŸå§‹æ–‡ä»¶ (Zstdå‹ç¼©);
    å‰ç«¯è´Ÿè´£è§£å‹ã€‚

    Args:
        file_id: æ–‡ä»¶ ID;

    Returns:
        FileResponse: .raw.zst æ–‡ä»¶;
    """
    from fastapi.responses import FileResponse
    file = crud.get_file(session, file_id)
    if not file:
        raise HTTPException(404, "File not found")

    raw_path = StorageService.get_raw_path(file.file_hash)
    if not raw_path.exists():
        raise HTTPException(404, "Raw file not found")

    # æ„å»ºä¸‹è½½æ–‡ä»¶å: filename(suffix).raw.zst
    # ä¾‹å¦‚: data.rawdata -> data (1).rawdata.zst
    base_name = file.filename
    suffix = file.name_suffix or ""
    
    # ç®€å•çš„æ‹¼æ¥é€»è¾‘ï¼šå‡è®¾ filename åŒ…å«æ‰©å±•å .rawdata
    # å¦‚æœ suffix å­˜åœ¨ï¼Œæ’å…¥åˆ°æ‰©å±•åä¹‹å‰? 
    # ç”¨æˆ·éœ€æ±‚: filename="data.rawdata", name_suffix=" (1)" -> "data (1).rawdata"
    
    if suffix:
        if base_name.lower().endswith(".rawdata"):
            stem = base_name[:-8] # remove .rawdata
            final_name = f"{stem}{suffix}.rawdata.zst"
        else:
             # å¦‚æœä¸æ˜¯ .rawdata ç»“å°¾, ç›´æ¥è¿½åŠ 
            final_name = f"{base_name}{suffix}.zst"
    else:
        final_name = f"{base_name}.zst"

    return FileResponse(
        path=raw_path,
        filename=final_name,
        media_type="application/zstd"
    )


@router.post("/files/batch-download")
def batch_download(
    request: api_models.BatchDownloadRequest,
    session: Session = Depends(deps.get_db)
):
    """
    æ‰¹é‡ä¸‹è½½æ–‡ä»¶ (ZipåŒ…);
    è¿”å›ä¸€ä¸ªåŒ…å«å¤šä¸ª .raw.zst æ–‡ä»¶çš„ Zip åŒ….

    Args:
        request: åŒ…å«è¦ä¸‹è½½çš„æ–‡ä»¶ ID åˆ—è¡¨;

    Returns:
        FileResponse: ä¸´æ—¶ Zip æ–‡ä»¶;
    """
    import tempfile
    import zipfile
    from fastapi.responses import FileResponse
    
    # 1. è·å–æ‰€æœ‰è¯·æ±‚çš„æ–‡ä»¶ä¿¡æ¯
    files_to_download = []
    for fid in request.ids:
        file = crud.get_file(session, fid)
        if file:
            files_to_download.append(file)
            
    if not files_to_download:
        raise HTTPException(400, "No valid files found")
        
    try:
        # 2. åˆ›å»ºä¸´æ—¶ Zip æ–‡ä»¶
        # delete=False å› ä¸º FileResponse éœ€è¦è¯»å–å®ƒ, ä¹‹åç”± BackgroundTask æ¸…ç†?
        # æˆ–è€…ä½¿ç”¨ tempfile.NamedTemporaryFile å¹¶ä¾èµ– OS æ¸…ç† (ä½†åœ¨ Windows ä¸Š FileResponse æ‰“å¼€æ—¶å¯èƒ½é”ä½)
        # æ›´å¥½çš„æ–¹å¼æ˜¯æ¯æ¬¡è¯·æ±‚ç”Ÿæˆä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ï¼Œä¾é  FileResponse(background=...) æ¸…ç†
        
        tmp_zip = tempfile.NamedTemporaryFile(delete=False, suffix=".zip", prefix="batch_")
        tmp_zip.close() # å…³é—­å¥æŸ„ï¼Œè®© zipfile æ‰“å¼€
        
        with zipfile.ZipFile(tmp_zip.name, 'w', zipfile.ZIP_STORED) as zf:
            for file in files_to_download:
                raw_path = StorageService.get_raw_path(file.file_hash)
                if not raw_path.exists():
                    logger.warning(f"Batch download skipping missing file: {file.id} ({file.file_hash})")
                    continue
                
                # æ„å»º Zip å†…çš„æ–‡ä»¶å
                base_name = file.filename
                suffix = file.name_suffix or ""
                
                if suffix:
                    if base_name.lower().endswith(".rawdata"):
                         # data.rawdata + (1) -> data (1).rawdata.zst
                        stem = base_name[:-8]
                        zip_entry_name = f"{stem}{suffix}.rawdata.zst"
                    else:
                        zip_entry_name = f"{base_name}{suffix}.zst"
                else:
                    zip_entry_name = f"{base_name}.zst"
                
                # æ·»åŠ åˆ° Zip
                zf.write(raw_path, arcname=zip_entry_name)
        
        # 3. è¿”å›å“åº”
        return FileResponse(
            path=tmp_zip.name,
            filename=f"sensorhub_batch_{datetime.now().strftime('%Y%m%d%H%M%S')}.zip",
            media_type="application/zip",
            background=BackgroundTask(lambda p: Path(p).unlink(missing_ok=True), tmp_zip.name)
        )
        
    except Exception as e:
        logger.error(f"Batch download error: {e}")
        raise HTTPException(500, f"Batch download failed: {str(e)}")


@router.post("/files/{file_id}/parse")
def trigger_parse(
    file_id: str,
    request: api_models.ParseRequest,
    session: Session = Depends(deps.get_db)
) -> dict:
    """
    è§¦å‘æ–‡ä»¶è§£æ;

    Args:
        file_id: æ–‡ä»¶ ID;
        request: è§£æé€‰é¡¹;

    Returns:
        dict: è§£æçŠ¶æ€;

    Raises:
        HTTPException: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º 404 é”™è¯¯;
    """
    file = crud.get_file(session, file_id)
    if not file:
        raise HTTPException(404, "File not found")

    # ç›´æ¥è®¾ç½®çŠ¶æ€ä¸ºå·²å¤„ç† (ç®€åŒ–çŠ¶æ€ç®¡ç†, æ—  Processing çŠ¶æ€)
    crud.update_file(session, file_id, {"status": "Processed"})

    return {"status": "Processed", "message": "Parse completed"}


@router.get("/files/{file_id}/content")
async def get_file_content_stream(
    file_id: str,
    session: Session = Depends(deps.get_db)
):
    """
    è·å–æ–‡ä»¶å‹ç¼©æ•°æ®æµï¼ˆå‰ç«¯è§£å‹ï¼‰;
    
    è¿”å›åŸå§‹çš„ .zst å‹ç¼©æ–‡ä»¶ï¼Œå‰ç«¯ä½¿ç”¨ zstd-wasm è§£å‹;
    
    Returns:
        StreamingResponse: .zst å‹ç¼©æ•°æ®æµ
        
    Headers:
        - Content-Type: application/zstd
        - X-File-Name: åŸå§‹æ–‡ä»¶å
        - X-Original-Size: åŸå§‹æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        - X-Compressed-Size: å‹ç¼©æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    Raises:
        HTTPException: æ–‡ä»¶ä¸å­˜åœ¨(404)ã€ç‰©ç†æ–‡ä»¶ç¼ºå¤±(404);
    """
    from fastapi.responses import StreamingResponse
    import os
    
    # è·å–æ–‡ä»¶è®°å½•
    file = crud.get_file(session, file_id)
    if not file:
        raise HTTPException(404, "File not found")
    
    # è·å–ç‰©ç†æ–‡ä»¶è·¯å¾„
    raw_path = StorageService.get_raw_path(file.file_hash)
    if not raw_path.exists():
        logger.error(f"Physical file not found for {file_id}: {raw_path}")
        raise HTTPException(404, "Physical file not found")
    
    # è·å–æ–‡ä»¶å¤§å°
    compressed_size = os.path.getsize(raw_path)
    
    # è·å–åŸå§‹å¤§å°ï¼ˆä» SensorFile.file_size_bytesï¼‰
    original_size = file.file_size_bytes if file.file_size_bytes > 0 else compressed_size
    
    # å®šä¹‰æµå¼ç”Ÿæˆå™¨
    async def stream_compressed_file():
        """æµå¼è¯»å–å‹ç¼©æ–‡ä»¶"""
        chunk_size = 64 * 1024  # 64KB chunks
        with open(raw_path, 'rb') as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                yield chunk
    
    # è¿”å›æµå¼å“åº”
    return StreamingResponse(
        stream_compressed_file(),
        media_type="application/zstd",
        headers={
            "X-File-Name": file.filename,
            "X-Original-Size": str(original_size),
            "X-Compressed-Size": str(compressed_size),
            "Content-Disposition": f'inline; filename="{file.filename}.zst"'
        }
    )
